{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da5a7da-3c93-4c28-8307-344c9603d804",
   "metadata": {},
   "source": [
    "## Setup jupyter notebook and install dependencies\n",
    "\n",
    "```\n",
    "python -m venv myenv\n",
    "source myenv/bin/activate\n",
    "pip install ipykernel\n",
    "pip install jupyter\n",
    "python -m ipykernel install --user --name=myenv\n",
    "jupyter notebook --ip 0.0.0.0 --port 8888\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65f9b0-7d52-4647-b73d-cca8fc9a6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic\n",
    "!pip install \"nbformat>=4.2.0\"\n",
    "!pip install tf-keras\n",
    "!pip install openpyxl\n",
    "#!pip install ipywidgets\n",
    "!pip install xlsxwriter\n",
    "#!pip install amphi-etl \n",
    "!pip install spacy\n",
    "!pip install pandas\n",
    "#!python -m spacy download sv_core_news_sm\n",
    "#!python -m spacy download sv_core_news_md\n",
    "!python -m spacy download sv_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from miniArkivet import miniArkivet\n",
    "import multi_train\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15fe14",
   "metadata": {},
   "source": [
    "## Parser and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e157265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miniArkivet parser\n",
    "from pathlib import Path\n",
    "Path(\"results/\").mkdir(parents=True, exist_ok=True)\n",
    "#df = miniArkivet(r\"dummy\") # 15000 rows\n",
    "#df = miniArkivet(r\"tests\") # only few rows\n",
    "df = miniArkivet(r\"C:\\Users\\frauker\\OneDrive - Chalmers\\Frauke\\Projects\\Agnotology of medical AI\\A-Media\\code\\2024-08-16_full-run\\case-1177\") #imports all txt files in the selected folder\n",
    "\n",
    "df['alltext'] = df[['title', 'text']].apply('\\n'.join, axis=1)\n",
    "#df.to_excel(\"results/parsed_results.xlsx\", sheet_name='results')\n",
    "df.to_csv(\"results/parsed_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf9ab0",
   "metadata": {},
   "source": [
    "# load and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_time = \"...\"\n",
    "df = pd.read_csv(\"results/parsed_results.csv\")\n",
    "# import re\n",
    "# Optional data cleanup here\n",
    "#df.text = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
    "#df.text = df.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
    "#df.text = df.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z]+\", \" \", row.text).split()), 1)\n",
    "\n",
    "# Source code generated by Amphi\n",
    "# Date: 2024-07-13 14:28:53\n",
    "# Additional dependencies: xlsxwriter\n",
    "#import pandas as pd\n",
    "\n",
    "# Reading data from 2024-07-12__18-43-parsed_results.csv\n",
    "csvFileInput1 = df\n",
    "\n",
    "# Deduplicate rows\n",
    "deduplicateData1 = csvFileInput1.drop_duplicates(subset=[\"title\", \"newspaper\", \"text\"])\n",
    "\n",
    "# Filter rows based on condition\n",
    "filter1 = deduplicateData1[~deduplicateData1['alltext'].str.contains(\"Ai Wei\", na=False)]\n",
    "#filter2 = filter1[~filter1['date'].str.contains(\"1993\", na=False)] \n",
    "\n",
    "#TODO: remove Eslövs AI, EAI\n",
    "# Filter rows based on condition\n",
    "filter3 = filter1.dropna(subset=['text', 'date']) #removes articles with empty text fields\n",
    "\n",
    "#TODO: remove empty date\n",
    "#filter2.to_excel(\"testresults.xlsx\", engine='xlsxwriter', header=True)\n",
    "  \n",
    "df = filter3\n",
    "#df.to_excel(\"results/cleaned_results.xlsx\", sheet_name='results')\n",
    "df.to_csv(\"results/cleaned_results.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2f6b90f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KBLab/megatron-bert-large-swedish-cased-165k\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"KBLab/robust-swedish-sentiment-multiclass\")\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19600ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sentence):\n",
    "    result = classifier(sentence)\n",
    "    return (result[0][\"label\"],result[0][\"score\"])\n",
    "\n",
    "#sentiment(\"En fin och glad mening.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['title_sent_label', 'title_sent_score']] = df['title'].apply(lambda t: pd.Series(sentiment(t)))\n",
    "df.to_csv(\"results/sent.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "725eed23",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde7122-5078-4493-beaf-cc79bd97d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"sv_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_persons = dict() \n",
    "dict_locations = dict()\n",
    "dict_orgs = dict()\n",
    "def NER(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    persons = []\n",
    "    orgs = []\n",
    "    locations = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_  == \"PRS\":\n",
    "            persons.append(ent.text)\n",
    "        elif ent.label_  == \"ORG\":\n",
    "            orgs.append(ent.text)\n",
    "        elif ent.label_  == \"LOC\":\n",
    "            locations.append(ent.text)\n",
    "    # remove duplicate items\n",
    "    orgs = list(set(orgs))\n",
    "    persons = list(set(persons))\n",
    "    locations = list(set(locations))\n",
    "    # Person\n",
    "    for item in persons:\n",
    "        if item  in dict_persons: \n",
    "            dict_persons[item] = dict_persons[item] + 1\n",
    "        else: \n",
    "            dict_persons[item] = 1\n",
    "\n",
    "    # Org\n",
    "    for item in orgs:\n",
    "        if item  in dict_orgs: \n",
    "            dict_orgs[item] = dict_orgs[item] + 1\n",
    "        else: \n",
    "            dict_orgs[item] = 1\n",
    "\n",
    "    # Location\n",
    "    for item in locations:\n",
    "        if item  in dict_locations: \n",
    "            dict_locations[item] = dict_locations[item] + 1\n",
    "        else: \n",
    "            dict_locations[item] = 1\n",
    "    return  (str(orgs), str(persons), str(locations))\n",
    "\n",
    "\n",
    "\n",
    "#sentence = \"Elon Musk och Steven Hawking pratar om AI på Migrationsverket\"\n",
    "#print(NER(sentence))\n",
    "\n",
    "df[['ner_orgs', 'ner_persons', 'ner_loc']] = df['alltext'].apply(lambda t: pd.Series(NER(t)))\n",
    "df.to_csv(\"results/ner.csv\", index=False)\n",
    "\n",
    "df_persons = pd.DataFrame(dict_persons.items(), columns=['Entity', 'Count'])\n",
    "df_persons.to_csv(\"results/person_count.csv\", index=False)\n",
    "\n",
    "df_locations = pd.DataFrame(dict_locations.items(), columns=['Entity', 'Count'])\n",
    "df_locations.to_csv(\"results/location_count.csv\", index=False)\n",
    "\n",
    "df_orgs = pd.DataFrame(dict_orgs.items(), columns=['Entity', 'Count'])\n",
    "df_orgs.to_csv(\"results/org_count.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95b8b80c",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae816e-6d91-496d-8ad8-6a0287dd5907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca976652",
   "metadata": {},
   "source": [
    "## Default bertTopic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75ec54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "SEstopwords = [\"aderton\",\"adertonde\",\"adjö\",\"aldrig\",\"alla\",\"allas\",\"allt\",\"alltid\",\"alltså\",\"andra\",\"andras\",\"annan\",\"annat\",\"artonde\",\"artonn\",\"att\",\"av\",\"bakom\",\"bara\",\"behöva\",\"behövas\",\"behövde\",\"behövt\",\"beslut\",\"beslutat\",\"beslutit\",\"bland\",\"blev\",\"bli\",\"blir\",\"blivit\",\"bort\",\"borta\",\"bra\",\"bäst\",\"bättre\",\"båda\",\"bådas\",\"dag\",\"dagar\",\"dagarna\",\"dagen\",\"de\",\"del\",\"delen\",\"dem\",\"den\",\"denna\",\"deras\",\"dess\",\"dessa\",\"det\",\"detta\",\"dig\",\"din\",\"dina\",\"dit\",\"ditt\",\"dock\",\"dom\",\"du\",\"där\",\"därför\",\"då\",\"e\",\"efter\",\"eftersom\",\"ej\",\"elfte\",\"eller\",\"elva\",\"emot\",\"en\",\"enkel\",\"enkelt\",\"enkla\",\"enligt\",\"ens\",\"er\",\"era\",\"ers\",\"ert\",\"ett\",\"ettusen\",\"fanns\",\"fem\",\"femte\",\"femtio\",\"femtionde\",\"femton\",\"femtonde\",\"fick\",\"fin\",\"finnas\",\"finns\",\"fjorton\",\"fjortonde\",\"fjärde\",\"fler\",\"flera\",\"flesta\",\"fram\",\"framför\",\"från\",\"fyra\",\"fyrtio\",\"fyrtionde\",\"få\",\"får\",\"fått\",\"följande\",\"för\",\"före\",\"förlåt\",\"förra\",\"första\",\"genast\",\"genom\",\"gick\",\"gjorde\",\"gjort\",\"god\",\"goda\",\"godare\",\"godast\",\"gott\",\"gälla\",\"gäller\",\"gällt\",\"gärna\",\"gå\",\"går\",\"gått\",\"gör\",\"göra\",\"ha\",\"hade\",\"haft\",\"han\",\"hans\",\"har\",\"heller\",\"hellre\",\"helst\",\"helt\",\"henne\",\"hennes\",\"hit\",\"hon\",\"honom\",\"hundra\",\"hundraen\",\"hundraett\",\"hur\",\"här\",\"hög\",\"höger\",\"högre\",\"högst\",\"i\",\"ibland\",\"icke\",\"idag\",\"igen\",\"igår\",\"imorgon\",\"in\",\"inför\",\"inga\",\"ingen\",\"ingenting\",\"inget\",\"innan\",\"inne\",\"inom\",\"inte\",\"inuti\",\"ja\",\"jag\",\"jo\",\"ju\",\"just\",\"jämfört\",\"kan\",\"kanske\",\"knappast\",\"kom\",\"komma\",\"kommer\",\"kommit\",\"kr\",\"kunde\",\"kunna\",\"kunnat\",\"kvar\",\"legat\",\"ligga\",\"ligger\",\"lika\",\"likställd\",\"likställda\",\"lilla\",\"lite\",\"liten\",\"litet\",\"länge\",\"längre\",\"längst\",\"lätt\",\"lättare\",\"lättast\",\"långsam\",\"långsammare\",\"långsammast\",\"långsamt\",\"långt\",\"låt\",\"man\",\"med\",\"mej\",\"mellan\",\"men\",\"mer\",\"mera\",\"mest\",\"mig\",\"min\",\"mina\",\"mindre\",\"minst\",\"mitt\",\"mittemot\",\"mot\",\"mycket\",\"många\",\"måste\",\"möjlig\",\"möjligen\",\"möjligt\",\"möjligtvis\",\"ned\",\"nederst\",\"nedersta\",\"nedre\",\"nej\",\"ner\",\"ni\",\"nio\",\"nionde\",\"nittio\",\"nittionde\",\"nitton\",\"nittonde\",\"nog\",\"noll\",\"nr\",\"nu\",\"nummer\",\"när\",\"nästa\",\"någon\",\"någonting\",\"något\",\"några\",\"nån\",\"nånting\",\"nåt\",\"nödvändig\",\"nödvändiga\",\"nödvändigt\",\"nödvändigtvis\",\"och\",\"också\",\"ofta\",\"oftast\",\"olika\",\"olikt\",\"om\",\"oss\",\"på\",\"rakt\",\"redan\",\"rätt\",\"sa\",\"sade\",\"sagt\",\"samma\",\"sedan\",\"senare\",\"senast\",\"sent\",\"sex\",\"sextio\",\"sextionde\",\"sexton\",\"sextonde\",\"sig\",\"sin\",\"sina\",\"sist\",\"sista\",\"siste\",\"sitt\",\"sitta\",\"sju\",\"sjunde\",\"sjuttio\",\"sjuttionde\",\"sjutton\",\"sjuttonde\",\"själv\",\"sjätte\",\"ska\",\"skall\",\"skulle\",\"slutligen\",\"små\",\"smått\",\"snart\",\"som\",\"stor\",\"stora\",\"stort\",\"större\",\"störst\",\"säga\",\"säger\",\"sämre\",\"sämst\",\"så\",\"sådan\",\"sådana\",\"sådant\",\"ta\",\"tack\",\"tar\",\"tidig\",\"tidigare\",\"tidigast\",\"tidigt\",\"till\",\"tills\",\"tillsammans\",\"tio\",\"tionde\",\"tjugo\",\"tjugoen\",\"tjugoett\",\"tjugonde\",\"tjugotre\",\"tjugotvå\",\"tjungo\",\"tolfte\",\"tolv\",\"tre\",\"tredje\",\"trettio\",\"trettionde\",\"tretton\",\"trettonde\",\"två\",\"tvåhundra\",\"under\",\"upp\",\"ur\",\"ursäkt\",\"ut\",\"utan\",\"utanför\",\"ute\",\"va\",\"vad\",\"var\",\"vara\",\"varför\",\"varifrån\",\"varit\",\"varje\",\"varken\",\"vars\",\"varsågod\",\"vart\",\"vem\",\"vems\",\"verkligen\",\"vi\",\"vid\",\"vidare\",\"viktig\",\"viktigare\",\"viktigast\",\"viktigt\",\"vilka\",\"vilkas\",\"vilken\",\"vilket\",\"vill\",\"väl\",\"vänster\",\"vänstra\",\"värre\",\"vår\",\"våra\",\"vårt\",\"än\",\"ännu\",\"är\",\"även\",\"åt\",\"åtminstone\",\"åtta\",\"åttio\",\"åttionde\",\"åttonde\",\"över\",\"övermorgon\",\"överst\",\"övre\"]\n",
    "vectorizer_model = CountVectorizer(stop_words=SEstopwords)\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with \n",
    "# a `bertopic.representation` model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "topic_model = BERTopic(\n",
    "      embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "      umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "      hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "      vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "      ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "      representation_model=representation_model # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    )\n",
    "topics, probs = topic_model.fit_transform(df.text)\n",
    "topic_model.save(\"results/defaultBertTopic.pickle\", serialization=\"pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ba0d6-4f5c-498a-9d88-8068518a8e96",
   "metadata": {},
   "source": [
    "## Build all models (optional)\n",
    "\n",
    "To see configuration for each model, please see `multi_train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b6963-864d-420c-b825-188b8326befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"results/ner.csv\")\n",
    "#docs = df['alltext'].tolist()\n",
    "#multi_train.build_all_models(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2533c",
   "metadata": {},
   "source": [
    "# Load model from file without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30344e-fa2d-47f8-973e-23dd2b7f6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "##list available models\n",
    "#from bertopic import BERTopic\n",
    "#print(glob.glob('./results/*.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7a534-14fc-4cbb-ad50-d2112d4f2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##it might cause problems to reload the topic model, preferred use is to run everything in the jupyter notebook again\n",
    "#topic_model_file = \"results/defaultBertTopic.pickle\"\n",
    "#topic_model= BERTopic.load(topic_model_file)\n",
    "#freq = topic_model.get_topic_info(); freq.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed49036-a21d-4fba-ba46-e4f4ed8b97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.get_topic(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4d731-fba6-4736-ae5f-e85d18924572",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda5a96-d4d8-4671-8d5f-984820dd7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_topics(); fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec82c3-0610-4c8f-a79b-878392976797",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(df['alltext'].tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe05703",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_term_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76510cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = df.date.to_list()\n",
    "alltext = df.alltext.to_list()\n",
    "print(len(timestamps), len(alltext))\n",
    "topics_over_time = topic_model.topics_over_time(alltext, timestamps, nr_bins=20)\n",
    "topic_model.visualize_topics_over_time(topics_over_time, topics=[10,11,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all documents and their corresponding topic\n",
    "result = topic_model.get_document_info(df.text)\n",
    "result = result.drop(['Representation', 'Representative_Docs' ,'Top_n_words', 'Representative_document' ], axis=1)\n",
    "result.to_csv('results/topic-docs.csv', index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c531d-4cd2-4d06-9a33-cb24d51a8690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
